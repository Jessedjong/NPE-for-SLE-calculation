{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb42d0b5-8e7e-4266-8288-02ce331ee3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from scipy.stats import kendalltau\n",
    "import time\n",
    "from scipy.sparse.linalg import eigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6fda4bd-57d3-4a44-b732-f926b5ad3405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\AppData\\Local\\Temp\\ipykernel_17432\\3481262583.py:6: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  assets = pd.read_csv(\"C:/Users/jesse/Downloads/output_ml_students_twosets/student_export_1/swisstools_assets.csv\",sep=\",\")\n",
      "C:\\Users\\jesse\\AppData\\Local\\Temp\\ipykernel_17432\\3481262583.py:16: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  assets3 = pd.read_csv(\"C:/Users/jesse/Downloads/output_ml_students/output_ml_students/backend_asset.csv\",sep=\",\")\n"
     ]
    }
   ],
   "source": [
    "# Import the three datasets\n",
    "# Dataset 1\n",
    "df = pd.read_csv(\"C:/Users/jesse/Downloads/output_ml_students_twosets/student_export_1/swisstools_asset_relationships.csv\",sep=\",\")\n",
    "relationships = df[[\"source_asset_id\",\"target_asset_id\"]]\n",
    "types = pd.read_csv(\"C:/Users/jesse/Downloads/output_ml_students_twosets/student_export_1/swisstools_asset_types.csv\",sep=\",\")\n",
    "assets = pd.read_csv(\"C:/Users/jesse/Downloads/output_ml_students_twosets/student_export_1/swisstools_assets.csv\",sep=\",\")\n",
    "# Dataset 2\n",
    "df2 = pd.read_csv(\"C://Users/jesse/Downloads/output_ml_students_twosets/student_export_2/haarlem_asset_relationships.csv\",sep=\",\")\n",
    "relationships2 = df2[[\"source_asset_id\",\"target_asset_id\"]]\n",
    "types2 = pd.read_csv(\"C:/Users/jesse/Downloads/output_ml_students_twosets/student_export_2/haarlem_asset_types.csv\",sep=\",\")\n",
    "assets2 = pd.read_csv(\"C:/Users/jesse/Downloads/output_ml_students_twosets/student_export_2/haarlem_assets.csv\",sep=\",\")\n",
    "# Dataset 3\n",
    "df3 = pd.read_csv(\"C:/Users/jesse/Downloads/output_ml_students/output_ml_students/backend_assetrelationship.csv\",sep=\",\")\n",
    "relationships3 = df3[[\"source_asset_id\",\"target_asset_id\"]]\n",
    "types3 = pd.read_csv(\"C:/Users/jesse/Downloads/output_ml_students/output_ml_students/backend_assettype.csv\",sep=\",\")\n",
    "assets3 = pd.read_csv(\"C:/Users/jesse/Downloads/output_ml_students/output_ml_students/backend_asset.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfb1e13-d1f8-4f40-b3e3-b9f4d87e7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the asset_and_type dataframe for each dataset\n",
    "# Dataset 1\n",
    "asset_and_type = assets.merge(types[[\"id\",\"category\"]],left_on=\"asset_type_id\",right_on=\"id\",how=\"left\")\n",
    "asset_and_type = asset_and_type.drop(columns=\"id_y\").rename(columns={\"id_x\":\"id\"})\n",
    "asset_and_type[\"category\"] = asset_and_type[\"category\"].fillna(\"Unspecified\")\n",
    "assettypes_1 = asset_and_type[[\"id\",\"category\"]]\n",
    "# Dataset 2\n",
    "asset_and_type_2 = assets2.merge(types2[[\"id\",\"category\"]],left_on=\"asset_type_id\",right_on=\"id\",how=\"left\")\n",
    "asset_and_type_2 = asset_and_type_2.drop(columns=\"id_y\").rename(columns={\"id_x\":\"id\"})\n",
    "asset_and_type_2[\"category\"] = asset_and_type_2[\"category\"].fillna(\"Unspecified\")\n",
    "assettypes_2 = asset_and_type_2[[\"id\",\"category\"]]\n",
    "# Dataset 3\n",
    "asset_and_type_3 = assets3.merge(types3[[\"id\",\"category\"]],left_on=\"asset_type_id\",right_on=\"id\",how=\"left\")\n",
    "asset_and_type_3 = asset_and_type_3.drop(columns=\"id_y\").rename(columns={\"id_x\":\"id\"})\n",
    "asset_and_type_3[\"category\"] = asset_and_type_3[\"category\"].fillna(\"Unspecified\")\n",
    "assettypes_3 = asset_and_type_3[[\"id\",\"category\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d9b04b-d754-4c8c-aad5-cda7d779b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiGraph is directed, Graph is undirected\n",
    "G1 = nx.DiGraph()\n",
    "edges = list(zip(relationships[\"source_asset_id\"],relationships[\"target_asset_id\"]))\n",
    "G1.add_edges_from(edges)\n",
    "# Second dataset\n",
    "G2 = nx.DiGraph()\n",
    "edges2 = list(zip(relationships2[\"source_asset_id\"],relationships2[\"target_asset_id\"]))\n",
    "G2.add_edges_from(edges2)\n",
    "# Third dataset\n",
    "G3 = nx.DiGraph()\n",
    "edges3 = list(zip(relationships3[\"source_asset_id\"],relationships3[\"target_asset_id\"]))\n",
    "G3.add_edges_from(edges3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73896b3c-c528-496b-9d5f-c007c0bb80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the second order neighbors counts of each node\n",
    "def second_order_neighbor_counts(G):\n",
    "    adj = G.adj\n",
    "    result = {}\n",
    "    for node in G:\n",
    "        direct = set(adj[node])\n",
    "        second_order = set()\n",
    "\n",
    "        for neighbor in direct:\n",
    "            for nn in adj[neighbor]:\n",
    "                if nn != node and nn not in direct:\n",
    "                    second_order.add(nn)\n",
    "        result[node] = len(second_order)\n",
    "    return result\n",
    "# Function to get the PE values\n",
    "def get_PE(G,second_order_counts):\n",
    "    # Clustering coefficient and cn\n",
    "    c = {}\n",
    "    for i in G.nodes:\n",
    "        if G.degree(i) <= 1:\n",
    "            c[i] = 0\n",
    "        else:\n",
    "            c[i] = (sum(1 for j in list(G.neighbors(i)) for k in list(G.neighbors(i)) if j != k and G.has_edge(j,k))) / (G.degree(i) * (G.degree(i) - 1))\n",
    "    cn = {}\n",
    "    sumcn = 0\n",
    "    for i in G.nodes:\n",
    "        sumneigh = len(list(G.neighbors(i))) + second_order_counts[i]\n",
    "        cn[i] = sumneigh/(1+c[i])\n",
    "        sumcn += cn[i]\n",
    "    # I\n",
    "    I = {}\n",
    "    for i in G.nodes:\n",
    "        I[i] = cn[i]/sumcn\n",
    "    # PE\n",
    "    sum_I = {}\n",
    "    PE = {}\n",
    "    for i in G.nodes:\n",
    "        sum_I[i] = 0.0\n",
    "        for j in G.neighbors(i):\n",
    "            if I[j] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                sum_I[i] += -I[j] * math.log(I[j])\n",
    "        PE[i] = sum_I[i]\n",
    "    return PE\n",
    "# Function to get all other measures, then put all together in a dictionary\n",
    "def get_measures(G,second_order_counts):\n",
    "    # Node Propagation Entropy\n",
    "    PE = get_PE(G=G,second_order_counts=second_order_counts)\n",
    "    # PageRank\n",
    "    pagerank = nx.pagerank(G)\n",
    "    # Degree Centrality\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    # H-index\n",
    "    h_index = {}\n",
    "    for node in G:\n",
    "        neighbor_degrees = sorted([G.degree(nbr) for nbr in G.neighbors(node)], reverse=True)\n",
    "        h = 0\n",
    "        for i, d in enumerate(neighbor_degrees, 1):\n",
    "            if d >= i:\n",
    "                h = i\n",
    "            else:\n",
    "                break\n",
    "        h_index[node] = h\n",
    "    # K-shell\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    k_shell = nx.core_number(G)\n",
    "    # Put them together in a dictionary\n",
    "    measures = {\"PE\":PE,\"PageRank\":pagerank,\"Degree Centrality\":degree_centrality,\"H-index\":h_index,\"K-shell\":k_shell}\n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ad296a8-2be0-4ac9-990f-83e20ff26aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the second order neighbor counts for each graph\n",
    "second_order_counts_1 = second_order_neighbor_counts(G1)\n",
    "second_order_counts_2 = second_order_neighbor_counts(G2)\n",
    "second_order_counts_3 = second_order_neighbor_counts(G3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab7d83bd-3b6a-4d36-9067-645ef7f043a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the function for each graph\n",
    "measures_1 = get_measures(G=G1,second_order_counts=second_order_counts_1)\n",
    "measures_2 = get_measures(G=G2,second_order_counts=second_order_counts_2)\n",
    "measures_3 = get_measures(G=G3,second_order_counts=second_order_counts_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b48fb41-2030-4ea7-b4b3-340f3b0cf25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate prevalence_threshold\n",
    "def get_prevalence_threshold(G):\n",
    "    A_sparse = nx.to_scipy_sparse_array(G, format='csr', dtype='float64')\n",
    "    lambda1 = eigs(A_sparse, k=1, which='LR', return_eigenvectors=False, maxiter=100000, tol=1e-3)[0].real\n",
    "    prevalence_threshold = 1 / lambda1\n",
    "    return prevalence_threshold\n",
    "# Function to run the SIR model\n",
    "def run_sir(G, seed, beta, gamma, max_steps=100):\n",
    "    infected = set([seed])\n",
    "    recovered = set()\n",
    "    susceptible = set(G.nodes) - infected\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        new_infected = set()\n",
    "        for node in infected:\n",
    "            for neighbor in G.neighbors(node):\n",
    "                if neighbor in susceptible and random.random() < beta:\n",
    "                    new_infected.add(neighbor)\n",
    "            if random.random() < gamma:\n",
    "                recovered.add(node)\n",
    "        infected = (infected - recovered).union(new_infected)\n",
    "        susceptible -= new_infected\n",
    "        if not infected:\n",
    "            break\n",
    "\n",
    "    return len(recovered)\n",
    "# Function to get the spread results\n",
    "def get_sir_results(measures, G, top_k, num_simulations, beta, gamma):\n",
    "    spread_results = {}\n",
    "    for name, vector in measures.items():\n",
    "        top_nodes = sorted(vector.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        top_nodes = [node for node, _ in top_nodes]\n",
    "        influence_scores = []\n",
    "        for node in top_nodes:\n",
    "            total_spread = 0\n",
    "            for _ in range(num_simulations):\n",
    "                spread = run_sir(G=G, seed=node, beta=beta, gamma=gamma)\n",
    "                total_spread += spread\n",
    "            avg_spread = total_spread / num_simulations\n",
    "            influence_scores.append(avg_spread)\n",
    "        spread_results[name] = influence_scores\n",
    "    return spread_results\n",
    "# Function to get kendall coefficients\n",
    "def kendall_scores(spread_results):\n",
    "    kendall_scores = {}\n",
    "    for name, influence_scores in spread_results.items(): \n",
    "        centrality_ranks = list(range(len(influence_scores)))\n",
    "        sir_ranks = sorted(range(len(influence_scores)), key=lambda i: influence_scores[i], reverse=True)\n",
    "        tau,_ = kendalltau(centrality_ranks, sir_ranks)\n",
    "        kendall_scores[name] = tau\n",
    "    return kendall_scores\n",
    "# Function to run the sis model\n",
    "def run_sis(G, seed, beta, gamma, max_steps=100):\n",
    "    infected = set([seed])\n",
    "    susceptible = set(G.nodes)\n",
    "    susceptible.remove(seed)\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        new_infected = set()\n",
    "        for node in infected:\n",
    "            for neighbor in G.neighbors(node):\n",
    "                if neighbor in susceptible and random.random() < beta:\n",
    "                    new_infected.add(neighbor)\n",
    "        # Infected nodes may recover (become susceptible again)\n",
    "        still_infected = {node for node in infected if random.random() >= gamma}\n",
    "        infected = still_infected.union(new_infected)\n",
    "        susceptible -= new_infected\n",
    "        susceptible |= (infected - still_infected)\n",
    "\n",
    "        if not infected:\n",
    "            break\n",
    "\n",
    "    return len(set(G.nodes)) - len(susceptible)\n",
    "# Function to get the sis results\n",
    "def get_sis_results(measures, G, top_k, num_simulations, beta, gamma):\n",
    "    spread_results = {}\n",
    "    for name, vector in measures.items():\n",
    "        top_nodes = sorted(vector.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        top_nodes = [node for node, _ in top_nodes]\n",
    "        influence_scores = []\n",
    "        for node in top_nodes:\n",
    "            total_spread = 0\n",
    "            for _ in range(num_simulations):\n",
    "                spread = run_sis(G=G, seed=node, beta=beta, gamma=gamma)\n",
    "                total_spread += spread\n",
    "            avg_spread = total_spread / num_simulations\n",
    "            influence_scores.append(avg_spread)\n",
    "        spread_results[name] = influence_scores\n",
    "    return spread_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d5026be-c94c-4212-b5e6-3c97e29f226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the prevalence_threshold for each graph\n",
    "prevalence_threshold_1 = get_prevalence_threshold(G1)\n",
    "prevalence_threshold_2 = get_prevalence_threshold(G2)\n",
    "prevalence_threshold_3 = get_prevalence_threshold(G3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f90531d9-40a0-4dd6-9534-e140e7db6446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the SIR model and get the kendall coefficients for each graph\n",
    "spread_results_1 = get_sir_results(measures=measures_1, G=G1, top_k=50, num_simulations=1000, beta=prevalence_threshold_1, gamma=1)\n",
    "kendall_coefficients_1 = kendall_scores(spread_results=spread_results_1)\n",
    "spread_results_2 = get_sir_results(measures=measures_2, G=G2, top_k=50, num_simulations=1000, beta=prevalence_threshold_2, gamma=1)\n",
    "kendall_coefficients_2 = kendall_scores(spread_results=spread_results_2)\n",
    "spread_results_3 = get_sir_results(measures=measures_3, G=G3, top_k=50, num_simulations=1000, beta=prevalence_threshold_3, gamma=1)\n",
    "kendall_coefficients_3 = kendall_scores(spread_results=spread_results_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16a6988c-00b4-4647-afe7-8fba317eaa3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'PE': 0.746938775510204,\n",
       "  'PageRank': 0.4873469387755102,\n",
       "  'Degree Centrality': 0.6587755102040816,\n",
       "  'H-index': 0.6146938775510205,\n",
       "  'K-shell': 0.17387755102040817},\n",
       " {'PE': 0.8808163265306123,\n",
       "  'PageRank': 0.6,\n",
       "  'Degree Centrality': 0.7893877551020408,\n",
       "  'H-index': 0.42857142857142855,\n",
       "  'K-shell': 0.29959183673469386},\n",
       " {'PE': 0.7959183673469388,\n",
       "  'PageRank': 0.4971428571428571,\n",
       "  'Degree Centrality': 0.6457142857142858,\n",
       "  'H-index': 0.6555102040816326,\n",
       "  'K-shell': 0.25387755102040815})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendall_coefficients_1, kendall_coefficients_2, kendall_coefficients_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63b73404-e4e9-4193-b517-21221fd7be75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PE': 0.5510204081632653,\n",
       " 'PageRank': 0.30775510204081635,\n",
       " 'Degree Centrality': 0.41714285714285715,\n",
       " 'H-index': 0.6326530612244898,\n",
       " 'K-shell': 0.21959183673469387}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the SIS model and get the kendall coefficients for each graph\n",
    "SIS_results_1 = get_sis_results(measures=measures_1,G=G1,top_k=50,num_simulations=1000,beta=prevalence_threshold_1,gamma=0.8)\n",
    "kendall_coefficients_1 = kendall_scores(spread_results=SIS_results_1)\n",
    "SIS_results_2 = get_sis_results(measures=measures_2,G=G2,top_k=50,num_simulations=1000,beta=prevalence_threshold_2,gamma=0.8)\n",
    "kendall_coefficients_2 = kendall_scores(spread_results=SIS_results_2)\n",
    "SIS_results_3 = get_sis_results(measures=measures_3,G=G3,top_k=50,num_simulations=1000,beta=prevalence_threshold_3,gamma=0.8)\n",
    "kendall_coefficients_3 = kendall_scores(spread_results=SIS_results_3)\n",
    "kendall_coefficients_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37140ad7-1633-4f0e-b3b1-9ec51f84ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIS_results_1 = get_sis_results(measures=measures_1,G=G1,top_k=50,num_simulations=1000,beta=prevalence_threshold_1,gamma=0.8)\n",
    "kendall_coefficients_1 = kendall_scores(spread_results=SIS_results_1)\n",
    "kendall_coefficients_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4168f0b9-df26-4ec5-ba66-88f66ef1f115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'PE': nan,\n",
       "  'PageRank': nan,\n",
       "  'Degree Centrality': nan,\n",
       "  'H-index': nan,\n",
       "  'K-shell': nan},\n",
       " {'PE': nan,\n",
       "  'PageRank': nan,\n",
       "  'Degree Centrality': nan,\n",
       "  'H-index': nan,\n",
       "  'K-shell': nan},\n",
       " {'PE': nan,\n",
       "  'PageRank': nan,\n",
       "  'Degree Centrality': nan,\n",
       "  'H-index': nan,\n",
       "  'K-shell': nan})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendall_coefficients_1, kendall_coefficients_2, kendall_coefficients_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3d7d621-4a7c-4596-80f2-e58ca8095461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SLE_valuation(target_ratios,NPE,assettypes):\n",
    "    NPE_df = pd.DataFrame(NPE.items(),columns=[\"id\",\"value\"])\n",
    "    NPE_and_type = pd.merge(NPE_df,assettypes,on=\"id\",how=\"left\")\n",
    "    total_value = NPE_and_type[\"value\"].sum()\n",
    "    group_totals = NPE_and_type.groupby(\"category\")[\"value\"].sum()\n",
    "    scaling_factors = {t: (target_ratios[t] * total_value) / group_totals[t] for t in group_totals.index}\n",
    "    NPE_and_type[\"adjusted_value\"] = NPE_and_type.apply(lambda row: row[\"value\"] * scaling_factors.get(row[\"category\"],1), axis=1)\n",
    "    NPE_values = NPE_and_type[[\"id\",\"category\",\"adjusted_value\"]]\n",
    "    return NPE_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "172e4a13-b1a1-49dc-90e9-64a287aa9038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the NPE final valuations\n",
    "target_ratios = {\"OT\":0.40,\"IoT\":0.20,\"IT\":0.15,\"Other\":0.15,\"Network\":0.10,\"Unspecified\":0.0}\n",
    "valuations_1 = NPE_valuation(target_ratios=target_ratios,NPE=measures_1[\"PE\"],assettypes=assettypes_1)\n",
    "valuations_2 = NPE_valuation(target_ratios=target_ratios,NPE=measures_2[\"PE\"],assettypes=assettypes_2)\n",
    "valuations_3 = NPE_valuation(target_ratios=target_ratios,NPE=measures_3[\"PE\"],assettypes=assettypes_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e752cb93-a17e-4466-94e5-58e424d59234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'IT': 0.15,\n",
       "  'IoT': 0.19999999999999996,\n",
       "  'Network': 0.1,\n",
       "  'OT': 0.4,\n",
       "  'Other': 0.15,\n",
       "  'Unspecified': 0.0},\n",
       " {'IT': 0.25,\n",
       "  'IoT': 0.33333333333333337,\n",
       "  'Network': 0.16666666666666666,\n",
       "  'Other': 0.25,\n",
       "  'Unspecified': 0.0},\n",
       " {'IT': 0.15,\n",
       "  'IoT': 0.2,\n",
       "  'Network': 0.1,\n",
       "  'OT': 0.4,\n",
       "  'Other': 0.15,\n",
       "  'Unspecified': 0.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the percentages\n",
    "# Dataset 1\n",
    "total_values_1 = dict(valuations_1.groupby(\"category\")[\"adjusted_value\"].sum())\n",
    "percentages_1 = {}\n",
    "for i in total_values_1:\n",
    "    percentages_1[i] = total_values_1[i]/sum(total_values_1.values())\n",
    "# Dataset 2\n",
    "total_values_2 = dict(valuations_2.groupby(\"category\")[\"adjusted_value\"].sum())\n",
    "percentages_2 = {}\n",
    "for i in total_values_2:\n",
    "    percentages_2[i] = total_values_2[i]/sum(total_values_2.values())\n",
    "# Dataset 3\n",
    "total_values_3 = dict(valuations_3.groupby(\"category\")[\"adjusted_value\"].sum())\n",
    "percentages_3 = {}\n",
    "for i in total_values_3:\n",
    "    percentages_3[i] = total_values_3[i]/sum(total_values_3.values())\n",
    "percentages_1, percentages_2, percentages_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17f8728b-e3be-4a81-8a41-d3b1ccbd1245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping line 1: ['%', 'bip', 'unweighted'] -> invalid literal for int() with base 10: '%'\n",
      "Skipping line 2: ['%', '1476', '829', '551'] -> invalid literal for int() with base 10: '%'\n",
      "Skipping line 1: ['%', 'sym', 'unweighted'] -> invalid literal for int() with base 10: '%'\n",
      "Skipping line 1: ['%', 'sym', 'unweighted'] -> invalid literal for int() with base 10: '%'\n",
      "Skipping line 1: ['%', 'sym', 'posweighted'] -> invalid literal for int() with base 10: '%'\n",
      "Skipping line 2: ['%', '243', '64', '64'] -> invalid literal for int() with base 10: '%'\n",
      "Skipping line 1: ['%', 'sym', 'unweighted'] -> invalid literal for int() with base 10: '%'\n",
      "Skipping line 1: ['%', 'sym', 'unweighted'] -> invalid literal for int() with base 10: '%'\n",
      "Skipping line 2: ['%', '6594', '4941', '4941'] -> invalid literal for int() with base 10: '%'\n",
      "Skipping line 1: ['%', 'sym', 'unweighted'] -> invalid literal for int() with base 10: '%'\n",
      "Skipping line 2: ['%', '2277', '1870', '1870'] -> invalid literal for int() with base 10: '%'\n"
     ]
    }
   ],
   "source": [
    "# Test on the extra datasets found in Yu et al.\n",
    "file_path_crime = \"C:/Users/jesse/Downloads/download.tsv.moreno_crime/moreno_crime/out.moreno_crime_crime\"\n",
    "G_crime = nx.DiGraph()\n",
    "file_path_netscience = \"C:/Users/jesse/Downloads/download.tsv.dimacs10-netscience/dimacs10-netscience/out.dimacs10-netscience\"\n",
    "G_netscience = nx.DiGraph()\n",
    "file_path_polbooks = \"C:/Users/jesse/Downloads/download.tsv.dimacs10-polbooks/dimacs10-polbooks/out.dimacs10-polbooks\"\n",
    "G_polbooks = nx.DiGraph()\n",
    "file_path_train = \"C:/Users/jesse/Downloads/download.tsv.moreno_train/moreno_train/out.moreno_train_train\"\n",
    "G_train = nx.DiGraph()\n",
    "file_path_dolphins = \"C:/Users/jesse/Downloads/download.tsv.dolphins/dolphins/out.dolphins\"\n",
    "G_dolphins = nx.DiGraph()\n",
    "file_path_uspowergrid = \"C:/Users/jesse/Downloads/download.tsv.opsahl-powergrid/opsahl-powergrid/out.opsahl-powergrid\"\n",
    "G_uspowergrid = nx.DiGraph()\n",
    "file_path_yeast = \"C:/Users/jesse/Downloads/download.tsv.moreno_propro/moreno_propro/out.moreno_propro_propro\"\n",
    "G_yeast = nx.DiGraph()\n",
    "data = {\"Crime\":(G_crime,file_path_crime),\n",
    "          \"Netscience\":(G_netscience,file_path_netscience),\n",
    "          \"Polbooks\":(G_polbooks,file_path_polbooks),\n",
    "          \"Train\":(G_train,file_path_train),\n",
    "          \"Dolphins\":(G_dolphins,file_path_dolphins),\n",
    "          \"Uspowergrip\":(G_uspowergrid,file_path_uspowergrid),\n",
    "          \"Yeast\":(G_yeast,file_path_yeast)}\n",
    "\n",
    "for i in data:  \n",
    "    graph = data[i][0]\n",
    "    file_path = data[i][1]\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            parts = line.strip().split()\n",
    "            try:\n",
    "                if len(parts) >= 2:\n",
    "                    from_node = int(parts[0])\n",
    "                    to_node = int(parts[1])\n",
    "                    edge_data = {}\n",
    "\n",
    "                    if len(parts) >= 3:\n",
    "                        edge_data['weight'] = float(parts[2])\n",
    "                    if len(parts) == 4:\n",
    "                        edge_data['timestamp'] = int(parts[3])\n",
    "                    graph.add_edge(from_node, to_node, **edge_data)\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping line {i}: {parts} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5081301f-69b4-49d3-a1c4-bf8ee0fcdcc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m measures \u001b[38;5;241m=\u001b[39m get_measures(G\u001b[38;5;241m=\u001b[39mgraph,second_order_counts\u001b[38;5;241m=\u001b[39msecond_order_counts)\n\u001b[0;32m      7\u001b[0m prevalence_threshold \u001b[38;5;241m=\u001b[39m get_prevalence_threshold(graph)\n\u001b[1;32m----> 8\u001b[0m spread_results \u001b[38;5;241m=\u001b[39m get_sir_results(measures\u001b[38;5;241m=\u001b[39mmeasures,G\u001b[38;5;241m=\u001b[39mgraph,top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,num_simulations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,beta\u001b[38;5;241m=\u001b[39mprevalence_threshold,gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m kendall_coefficients \u001b[38;5;241m=\u001b[39m kendall_scores(spread_results\u001b[38;5;241m=\u001b[39mspread_results)\n\u001b[0;32m     10\u001b[0m scores[i] \u001b[38;5;241m=\u001b[39m kendall_coefficients\n",
      "Cell \u001b[1;32mIn[15], line 38\u001b[0m, in \u001b[0;36mget_sir_results\u001b[1;34m(measures, G, top_k, num_simulations, beta, gamma)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_simulations):\n\u001b[0;32m     37\u001b[0m     spread \u001b[38;5;241m=\u001b[39m run_sir(G\u001b[38;5;241m=\u001b[39mG, seed\u001b[38;5;241m=\u001b[39mnode, beta\u001b[38;5;241m=\u001b[39mbeta, gamma\u001b[38;5;241m=\u001b[39mgamma)\n\u001b[1;32m---> 38\u001b[0m     total_spread \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m spread\n\u001b[0;32m     39\u001b[0m avg_spread \u001b[38;5;241m=\u001b[39m total_spread \u001b[38;5;241m/\u001b[39m num_simulations\n\u001b[0;32m     40\u001b[0m influence_scores\u001b[38;5;241m.\u001b[39mappend(avg_spread)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graphs = {}\n",
    "for i in data:\n",
    "    graphs[i] = data[i][0]\n",
    "scores = {}\n",
    "for i in graphs:   \n",
    "    graph = graphs[i]\n",
    "    second_order_counts = second_order_neighbor_counts(graph)\n",
    "    measures = get_measures(G=graph,second_order_counts=second_order_counts)\n",
    "    prevalence_threshold = get_prevalence_threshold(graph)\n",
    "    spread_results = get_sir_results(measures=measures,G=graph,top_k=50,num_simulations=10000,beta=prevalence_threshold,gamma=1)\n",
    "    kendall_coefficients = kendall_scores(spread_results=spread_results)\n",
    "    scores[i] = kendall_coefficients\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "05204d69-9c09-4e7f-ab31-6177a25ec750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Crime': {'PE': 0.516734693877551,\n",
       "  'PageRank': -0.022040816326530613,\n",
       "  'Degree Centrality': 0.04816326530612245,\n",
       "  'H-index': 0.28,\n",
       "  'K-shell': 0.08081632653061226},\n",
       " 'Netscience': {'PE': 0.2457142857142857,\n",
       "  'PageRank': 0.7877551020408163,\n",
       "  'Degree Centrality': 0.26857142857142857,\n",
       "  'H-index': 0.18040816326530612,\n",
       "  'K-shell': 0.5151020408163265},\n",
       " 'Polbooks': {'PE': 0.43183673469387757,\n",
       "  'PageRank': -0.19020408163265307,\n",
       "  'Degree Centrality': 0.15918367346938775,\n",
       "  'H-index': 0.3142857142857143,\n",
       "  'K-shell': 0.4416326530612245},\n",
       " 'Train': {'PE': 0.9004081632653061,\n",
       "  'PageRank': -0.16408163265306122,\n",
       "  'Degree Centrality': 0.4759183673469388,\n",
       "  'H-index': 0.7093877551020409,\n",
       "  'K-shell': 0.6293877551020408},\n",
       " 'Dolphins': {'PE': 0.7485714285714286,\n",
       "  'PageRank': -0.2653061224489796,\n",
       "  'Degree Centrality': 0.005714285714285714,\n",
       "  'H-index': 0.2440816326530612,\n",
       "  'K-shell': -0.40244897959183673},\n",
       " 'Yeast': {'PE': 0.47265306122448986,\n",
       "  'PageRank': 0.1689795918367347,\n",
       "  'Degree Centrality': 0.31755102040816324,\n",
       "  'H-index': 0.2571428571428571,\n",
       "  'K-shell': 0.030204081632653063}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs = {\"Crime\":G_crime,\"Netscience\":G_netscience,\"Polbooks\":G_polbooks,\"Train\":G_train,\"Dolphins\":G_dolphins,\"Yeast\":G_yeast}\n",
    "scores = {}\n",
    "for i in graphs:   \n",
    "    graph = graphs[i]\n",
    "    second_order_counts = second_order_neighbor_counts(graph)\n",
    "    measures = get_measures(G=graph,second_order_counts=second_order_counts)\n",
    "    prevalence_threshold = get_prevalence_threshold(graph)\n",
    "    spread_results = get_sis_results(measures=measures,G=graph,top_k=50,num_simulations=1000,beta=prevalence_threshold,gamma=0.8)\n",
    "    kendall_coefficients = kendall_scores(spread_results=spread_results)\n",
    "    scores[i] = kendall_coefficients\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
